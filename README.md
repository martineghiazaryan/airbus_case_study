# Airbus Ship Detection - Segmentation Model

## Table of Contents
1. [Introduction](#introduction)
2. [Prerequisites](#prerequisites)
3. [Folder Structure](#folder-structure) 
4. [Dataset](#dataset)
5. [Methodology](#methodology)
6. [Installation](#installation)
7. [Usage](#usage)
8. [Results](#results)
9. [Future Enhancements](#future-enhancements)
10. [References](#references)
11. [Contact](#contact)

## Introduction
<!-- Write about your project, why you chose it, and what problem it solves -->

## Prerequisites 

Before you can run the code in this repository, there are a few steps that you need to follow:

1. **Download the preprocessed data**: To save time on preprocessing, you can download the preprocessed `X_train`, `y_train`, `X_valid`, and `y_valid` datasets directly from this [link](https://drive.google.com/file/d/1bEcS5kJSgKq3-Xg4FpWQAfjKK3z1S3sR/view?usp=sharing). Once downloaded, please locate these files in the root directory of this project.

2. **Download the original dataset**: The original dataset, along with the images, can be downloaded from the following Kaggle competition - [Airbus Ship Detection](https://www.kaggle.com/c/airbus-ship-detection/data). Please download and change the corresponding paths to the images folders in the code. 

Here is the folder structure for the dataset that you will need:

- üìÅ airbus-ship-detection
  - üìÅ test_v2
  - üìÅ train_v2
  - train_ship_segmentations_v2.csv


## Folder Structure

This project has a modular structure and is divided into several Python scripts and Jupyter notebooks for various tasks. The organization is as follows:

- üìÅ root
  - data_preprocessing.py
  - main.py
  - model_creation.py
  - model_inference.py
  - model_training.py
  - README.md
  - requirements.txt
  - train_df.csv
  - valid_df.csv
  - üìÅ .ipynb_checkpoints
    - Airbus Case study -checkpoint.ipynb
    - airbus-case-study (1)-checkpoint.ipynb
  - üìÅ models
    - model_best_checkpoint.h5
  - üìÅ Notebooks
    - airbus-ship-segment-everything.ipynb
    - airbus-ship-segmentation.ipynb
    - airbus-ship-segmentation_models.ipynb
  - üìÅ __pycache__
    - data_preprocessing.cpython-310.pyc
    - model_creation.cpython-310.pyc
    - model_inference.cpython-310.pyc
    - model_training.cpython-310.pyc


- `data_preprocessing.py`: This script contains all the necessary steps for preprocessing the Airbus Ship Detection Dataset. 

- `model_creation.py`: This script is used for defining and creating the segmentation model.

- `model_training.py`: This script is used for training the model on the preprocessed dataset.

- `model_inference.py`: This script is used for making predictions with the trained model.

- `main.py`: This is the main driver script that coordinates the running of the scripts mentioned above.

- `train_df.csv` and `valid_df.csv`: These are CSV files that contain the training and validation dataframes respectively.

- `requirements.txt`: This file lists all the Python dependencies required to run the project.

The repository also contains the following directories:

- `models`: This directory contains the saved model weights and architectures. Currently, it contains `model_best_checkpoint.h5`, which are the weights of the best model checkpoint during training. This will help you save time and not train the model. Just load the model and predict. See the instructions.

- `Notebooks`: This directory contains Jupyter notebooks some additional workflows that I did except for the task which was to train the U-Net model. It currently includes:
    - `airbus-ship-segment-everything.ipynb`: This notebook includes the recent Segment-Everything model from Meta AI. In integrated the model and adjusted it to do binary classification in our case of ship-segmentation.
    - `airbus-ship-segmentation.ipynb`: This notebook is the same model version that I wrote in this repository only in the version of kaggle notebook.
    - `airbus-ship-segmentation_models.ipynb`: This notebook presents another aproach that I tried which is to use a pre-trained encoder based on resnet34 also using and experimenting with the **segmentation_model** library.
  
- `.ipynb_checkpoints`: This directory contains checkpoint files from Jupyter notebooks, which are created while the notebook is open. 

- `__pycache__`: This directory contains compiled Python scripts, generated by Python interpreter for performance optimization.

- `README.md`: This is the file you're reading now. It provides an overview of the project and explains how to use it.


## Dataset

The data used in this project comes from the [Airbus Ship Detection Challenge](https://www.kaggle.com/c/airbus-ship-detection/data) hosted on Kaggle. The challenge of this competition is to detect ships in satellite images as quickly as possible.

The dataset is structured as follows:

- The `train_v2` folder contains a set of jpg images, each of which can contain multiple ships. 
- The `test_v2` folder contains jpg images in the public test set.
- The `train_ship_segmentations_v2.csv` file provides the run-length encoded pixel locations of the ship for the training images. 

The images were acquired by Airbus' Pleiades satellites, which are capable of detailed imaging with a resolution of up to 0.5 meters.

Each image can contain multiple ships or no ships at all, and the task is to identify the presence of ships in these images and also locate them. For more detailed information about the dataset, please refer to the competition's [official webpage](https://www.kaggle.com/c/airbus-ship-detection/data).


## Methodology

This project utilizes a step-by-step approach to detect ships in satellite images, consisting of data cleaning and preprocessing, model development, training, and evaluation.

### Data Cleaning & Preprocessing

The data preprocessing includes the loading and filtering of the initial dataset, which is located in the `train_ship_segmentations_v2.csv` file. The first 5000 images are kept for processing. This dataset is then divided into two parts: images with ships (`ships_df`) and images without ships (`no_ships_df`). Each part is further split into training and validation datasets, with 80% for training and 20% for validation.

The script also includes a `preprocess_data` function for resizing images and masks and converting the masks to categorical format. A `preprocess_test_data` function is included for resizing the test images.

The resulting datasets are then saved as `train_df.csv` and `valid_df.csv` for further use.

### Model Development

The model used in this project is a custom TensorFlow model defined in `model_creation.py`. This model uses a custom metric function `dice_coef`, defined in `model_training.py`.

### Training

The training of the model is performed in the `model_training.py` module. The training process can be started from a previously trained model, or from scratch. By default, the code is set to load a previously trained model, but you can uncomment the line `mt.train_model(model)` to continue training from the loaded model. If you want to start training from scratch, call `mt.train_model()` without any arguments.

### Evaluation

The evaluation of the model is performed by the `model_inference.py` module. It uses the trained model to perform inference on a set of test images, located in the `test_img_dir` directory.

## Installation

To run this code, you'll need to have Python, TensorFlow, and OpenCV installed, along with the required libraries mentioned at the start of the script (numpy, pandas, sklearn, keras). You'll also need to have the `model_creation.py`, `model_training.py`, and `model_inference.py` scripts in the same directory.

## Usage

To use this code, simply run the script from the command line. Make sure to replace the `test_img_dir` and `model_path` with the paths to your test images directory and your trained model, respectively. The `model_path` is currently set to a .h5 file, which is the format used to store the trained TensorFlow model.

## Results

The results will be outputted by the `model_inference.py` module as it performs inference on the test images.

## Future Enhancements

Future enhancements to this project could include refining the model architecture for improved accuracy, expanding the training set to include more images, and implementing more complex data augmentation techniques to improve model generalization.


## References
<!-- List of resources and references used in your project -->

## Contact
<!-- Your contact information -->

