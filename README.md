# Airbus Ship Detection - Segmentation Model

## Table of Contents
1. [Introduction](#introduction)
2. [Prerequisites](#prerequisites)
3. [Folder Structure](#folder-structure)
4. [Dataset](#dataset)
5. [Methodology](#methodology)
6. [Installation](#installation)
7. [Usage](#usage)
8. [Results](#results)
9. [Future Enhancements](#future-enhancements)
10. [References](#references)
11. [Contact](#contact)

## Introduction
<!-- Write about your project, why you chose it, and what problem it solves -->

## Prerequisites
<!-- List the programming languages, libraries, and frameworks you used in this project -->

## Folder Structure

This project has a modular structure and is divided into several Python scripts and Jupyter notebooks for various tasks. The organization is as follows:

- üìÅ root
  - data_preprocessing.py
  - main.py
  - model_creation.py
  - model_inference.py
  - model_training.py
  - README.md
  - requirements.txt
  - train_df.csv
  - valid_df.csv
  - üìÅ .ipynb_checkpoints
    - Airbus Case study -checkpoint.ipynb
    - airbus-case-study (1)-checkpoint.ipynb
  - üìÅ models
    - model_best_checkpoint.h5
  - üìÅ Notebooks
    - airbus-ship-segment-everything.ipynb
    - airbus-ship-segmentation.ipynb
    - airbus-ship-segmentation_models.ipynb
  - üìÅ __pycache__
    - data_preprocessing.cpython-310.pyc
    - model_creation.cpython-310.pyc
    - model_inference.cpython-310.pyc
    - model_training.cpython-310.pyc


- `data_preprocessing.py`: This script contains all the necessary steps for preprocessing the Airbus Ship Detection Dataset. 

- `model_creation.py`: This script is used for defining and creating the segmentation model.

- `model_training.py`: This script is used for training the model on the preprocessed dataset.

- `model_inference.py`: This script is used for making predictions with the trained model.

- `main.py`: This is the main driver script that coordinates the running of the scripts mentioned above.

- `train_df.csv` and `valid_df.csv`: These are CSV files that contain the training and validation dataframes respectively.

- `requirements.txt`: This file lists all the Python dependencies required to run the project.

The repository also contains the following directories:

- `models`: This directory contains the saved model weights and architectures. Currently, it contains `model_best_checkpoint.h5`, which are the weights of the best model checkpoint during training. This will help you save time and not train the model. Just load the model and predict. See the instructions.

- `Notebooks`: This directory contains Jupyter notebooks some additional workflows that I did except for the task which was to train the U-Net model. It currently includes:
    - `airbus-ship-segment-everything.ipynb`: This notebook includes the recent Segment-Everything model from Meta AI. In integrated the model and adjusted it to do binary classification in our case of ship-segmentation.
    - `airbus-ship-segmentation.ipynb`: This notebook is the same model version that I wrote in this repository only in the version of kaggle notebook.
    - `airbus-ship-segmentation_models.ipynb`: This notebook presents another aproach that I tried which is to use a pre-trained encoder based on resnet34 also using and experimenting with the **segmentation_model** library.
  
- `.ipynb_checkpoints`: This directory contains checkpoint files from Jupyter notebooks, which are created while the notebook is open. 

- `__pycache__`: This directory contains compiled Python scripts, generated by Python interpreter for performance optimization.

- `README.md`: This is the file you're reading now. It provides an overview of the project and explains how to use it.


## Dataset
<!-- Describe the Airbus Ship Detection dataset and any preprocessing steps you took -->

## Methodology
<!-- Step-by-step description of your methods -->

### Data Cleaning & Preprocessing
<!-- Details about the preprocessing steps and any transformations done on the data -->

### Model Development
<!-- Details about the model used, why it was chosen, and how it was implemented -->

### Training
<!-- Details about how the model was trained, what kind of split was used for the training and validation sets -->

### Evaluation
<!-- Details about how the model's performance was evaluated -->

## Installation
<!-- Instructions for setting up and installing your project -->

## Usage
<!-- Instructions on how to use your code, run the scripts -->

## Results
<!-- Explanation of the results -->

## Future Enhancements
<!-- Any improvements that can be done or features that can be added in the future -->

## References
<!-- List of resources and references used in your project -->

## Contact
<!-- Your contact information -->

